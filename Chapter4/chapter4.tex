\chapter{Deep Neural Networks as Point Estimates for Deep Gaussian Processes}

\citet{mackay1992practical,mackay1992bmc} noted very early that the Bayesian treatment of neural networks had large potential. To start, it can help to quantify estimates of the error bars on the network outputs, but it can also provide an objective that can be used for the comparison of alternative network architectures. The reality, however, was that Bayesian inference in neural networks was --and still is-- challenging, and in practice one has to resort to either crude approximations (e.g., Laplace approximation \citep{MacKay1998laplace}) or lengthy computations (e.g., Markov Chain Monte Carlo \citep{neal1992bayesian}).

Building on the foundational work of MacKay, and driven by the amazing successes of large deterministic deep neural networks (DNNs), the literature has seen an upspring in the development of more scalable methods to perform approximate Bayesian inference in DNNs \citep{blundell2015,Kingma2015local,Gal2016dropout}. However, it remains challenging to encode prior assumptions on functions through distributions on weights and the large number of parameters to be estimated makes it computationally prohibitive. Furthermore, the strong approximations used both during modelling and inference make it unclear to what extent these models approximate the true posterior distribution \citep{hron2017variational, foong2019expressiveness}. They also do not deliver on an important promise of Bayesian methods: an approximate marginal likelihood objective that can be used for automatic model selection and hyperparameter learning. A different approach may thus be necessary to unlock the Bayesian benefits in deep learning.

\citet{neal1996bayesian} showed that for infinitely wide single-layer BNNs the distribution over the non-linear functions are given by Gaussian processes. \citet{williams1998computation,cho2009kernel} extended this theory and derived the kernel corresponding to an infinite-width BNN with Sigmoidal and ReLU activation function, respectively. The beauty of this connection is that performing Bayesian inference in the corresponding GP model can be done exactly and analytically -- all in a single elegant framework \citep{rasmussen2006}. Since, various approximations to the exact GP framework have been developed to allow for non-Gaussian likelihoods \citep{kuss2005assessing,hensman2013}, large datasets \citep{hensman2015scalable, wang2019exact}, and even neural network like structures such as convolutions \citep{vanderwilk2017conv}. Crucially, the approximations to the marginal likelihood still enable the main Bayesian benefits: model uncertainty and learning model hyperparameters (e.g., \citep{vanderwilk2018invariances,Dutordoir2020convolutional}).

More recently, \citet{matthews2018gaussian} discovered the equivalence between \emph{deep} (i.e. multi-layer) BNNs and GPs. This has led to the development of deep (fully connected and convolutional) neural network kernels for GPs (NN-GPs). Interestingly, the performance of the non-Bayesian neural nets significantly outperforms the corresponding GPs \citep{garriga2018deep,novak2018bayesian}. The discrepancy hints at the fact that these single-layer GPs, even when configured with very expressive DNN equivalent kernels, are missing a crucial component: the ability to learn feature representations from the data.
% From James' Variationally compressed DGPs: ``Both Neal [1996] and MacKay [1998] pointed out some of the limitations of priors that ensure joint Gaussianity across observations and this has inspired work in moving beyond Gaussian processes"

Deep Gaussian processes \citep[DGPs]{Damianou2013} are an interesting avenue to tackle these challenges. They are built by hierarchically stacking GPs on top of each other, which enables the learning of more powerful representations through compositional features. Moreover, their Bayesian approximations in function-space seem to be of higher quality than those of weight-space BNNs, e.g.~as supported by the successful use of marginal likelihood estimates for hyperparameter learning \citep{Damianou2013}.
% DGPs have been shown to be accurate and robust in the low-data regime, to allow for automatic model selection and hyperparameter learning, and to exhibit accurate predictive uncertainty \citep{Damianou2013, salimbeni2017doubly,Dutordoir2020convolutional}.

While promising, DGPs have struggled for relevance in applications due to the challenges and cost associated with Bayesian inference. Training DGPs is computationally expensive and requires very careful setting of the parameters. Furthermore, the hierarchical prior induced by naively stacking stationary kernels gives rise to pathological, ``collapsed'' samples \citet{duvenaud2014avoiding}. Considerable progress for scalable inference in DGPs was made by \citet{hensman2014nested, salimbeni2017doubly}, who derived stochastic variational lower bounds. The formulation of these bounds closely resembles the computations of training a feed-forward neural network with regularisation terms, and has greatly inspired this work.

Building on \citet{salimbeni2017doubly} and to further improve the scalability of DGPs, \citet{cutajar2017random} used a Random Fourier Feature \citep[RFF]{rahimi2008random} approximation of the kernel. While successful, this approach introduces an approximation in both the prior and the posterior of the model. More recently, \citet{rudner2020inter} proposed Fourier features of the Mat\'ern RKHS, following \citet[Variational Fourier Features (VFF)]{hensman2017variational}, to build inter-domain DGPs. Like for single layer VFF models, this approach can lead to faster training and improved performance, but is only computationally feasible for data of dimension one or two. In parallel with our work, \citet{sun2021neural} explored the idea of using single-layer neural networks to parameterise inducing points in shallow GPs. Similar to this paper, their method makes use of the spherical harmonic decomposition of a kernel. However, their works differs in the fact that they focus on shallow GPs, on bounded inducing functions (e.g.,  $\textrm{erf}$), and directly use the Nystr\"om approximation to approximate the model's uncertainty estimates rather than the ELBO to learn the variational parameters.

The analysis of (Bayesian) neural networks has led to several probabilistic models: NN-GPs, GPs and DGPs. In this work, however, rather than focusing on the \emph{prior} induced by these equivalent models, the emphasis lies on the connection between the DGP \emph{posterior} and the DNN. This has received much less attention in the literature, though we argue it is a more interesting regime to study. The connection between GP piors and neural nets is established only in the infinite limit of the number of hidden units. The sparse posterior DGP, on the contrary, is built out of a finite set of basis functions and can thus immediately be connected to finite-width neural networks --- in this paper we connect both models in their modus operandi.

We formulate a DGP configuration for which the approximate posterior mean has the same mathematical structure as a deep neural network with fully connected layers and non-linear activation functions. We can use this unification to train the DGP like a neural network which allow us to leverage all the great research in this area for DGP inference. Furthermore, this connection between DGPs posteriors and DNNs highlights the great potential of DGPs as a model for learning powerful representations from data while being fully Bayesian.