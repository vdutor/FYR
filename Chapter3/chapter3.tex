\chapter{Linear Time Gaussian Processes on Spheres}
\label{chapter:vish}

Stationary kernels, i.e. translation invariant covariances, are ubiquitous in machine learning when the input space is Euclidean. When working on the hypersphere, their spherical counterpart are dot-product kernels, which are invariant to rotations. They are the main object under study in this chapter. In \cref{sec:rkhs-dotproduct-kernels}, we first show how we can construct the Reproducing Kernel Hilbert Space (RKHS) of dot-product kernel on the hypersphere. \Cref{sec:vish} uses the RKHS to construct an efficient variational interdomain inducing variable for Gaussian processes on the hypersphere. We will then show how to expand the GP onto the complete domain $\Reals^d$ and conclude with a series of experiments, showing the speed and accuracy of the proposed approach.

\section{Mercer Representation of Dot-Product Kernels}
\label{sec:rkhs-dotproduct-kernels}

Consider the unit sphere in $\Reals^d$  as the input domain
\begin{equation}
    \mathcal{X} = \dsphere = \{x \in \Reals^d: \norm{x}_2 = 1\}
\end{equation}
and $\nu$ to be the Lebesgue measure on $\dsphere$, so that
\begin{equation}
    \darea = \int_{\dsphere} \calcd{\nu(x)} = \frac{2 \pi ^ {d/2}}{\Gamma(d/2)}.
\end{equation}
is the surface area of $\dsphere$. We adopt the usual $L_2$ inner product for functions $f: \dsphere \rightarrow \Reals$ and $g: \dsphere \rightarrow \Reals$ restricted to the sphere 
\begin{equation}
     \langle f, g\rangle_{L_{2}(\dsphere)} = \frac{1}{\darea} \int_{\dsphere} f(x)\,g(x) \, \calcd{\nu(x)}.
\end{equation}

We define a dot-product kernel, also known as a zonal kernel (we will use both terms interchangably), as a p.d. kernel of the form
\begin{equation}
    k(x, x') = \kappa(x\transpose x'),
\end{equation}
where $\kappa: [-1, 1] \rightarrow \Reals$ is a continuous function and referred to as the shape function. In other words, dot-product kernels only depend on the distance on the great circle between two inputs, rather than their location. They are the counterpart of stationary kernels $k(x, x') = \kappa(x - x')$, who are functions of the difference only. For example, where stationary kernels are translation invariant, dot-product kernel are rotationally invariant.

TODO: Arc Cosine and Stationary kernels on the hypersphere
% The first order Arc Cosine kernel mimics the computation of infinitely wide fully connected layers with ReLU activations. \citet{cho2009kernel} showed that for $\sigma(t) = \max(0, t)$, the covariance between function values of $f(\vx) = \sigma(\vw^\top \vx)$ for $\vw \sim \NormDist{0, d^{-1/2} \Eye_d}$ and $\vw \in \Reals^d$ is given by
% \begin{equation}
% \label{eq:arccosine}
%     k(\vx, \vx') = \Exp{\vw}{\sigma(\vw^\top \vx)\, \sigma(\vw^\top \vx')} = \underbrace{\norm{\vx} \norm{\vx'}}_{\text{radial}}\ \underbrace{\frac{1}{\pi}\big( \sqrt{1 - t^2} + t\, (\pi - \arccos t) \big)}_{\text{angular (shape function) } s(t)},
% \end{equation}
% where $t = \frac{\vx^\top \vx'}{\norm{\vx}\norm{\vx'}}$. The factorisation of the kernel in a radial and angular factor leads to an RKHS consisting of functions of the form $f(\vx) = \norm{\vx}\,g(\frac{\vx}{\norm{\vx}})$, where $g(\cdot)$ is defined on the unit hypersphere $\dsphere = \{\vx \in \Reals^d: \norm{\vx}_2 = 1\}$ but fully determines the function on $\Reals^d$.


We can associate a kernel operator $\mathcal{K}$ to a zonal kernel, as explained in \cref{section:theory:spectral-formulation}, which exhibits the form
\begin{equation}
    \label{eq:kernel-operator-zonal}
  \mathcal{K} f = \int_{\dsphere} \kappa(x\transpose\cdot) f(x) \calcd{\nu(x)}.
\end{equation}
To construct a Mercer representation of a zonal kernel's RKHS we require the eigensystem of the kernel operator $\mathcal{K}$, or, equivalently, the set of eigenfunctions $\{\phi_n\}$ and associated eigenvalues $\{\lambda_n\}$ for which
\begin{equation}
    \mathcal{K} \phi_n = \lambda_n \phi_n\qquad\text{and}\qquad \langle \phi_n, \phi_m \rangle_{L_2(\dsphere)} = \delta_{nm}.
\end{equation}

To obtain the eigensystem of $\mathcal{K}$, we will show that $\mathcal{K}$ commutes with the Laplace-Beltrami operator $\LaplaceBeltrami$, henceforth referred to as simply the Laplacian or the Laplace operator. We want to remind the reader that commuting operators share the same eigenfunctions, but not necessarily the same eigenvalues. Therefore, to find the eigenfunctions of the kernel operator $\mathcal{K}$, it suffice to find the eigenfunctions of the Laplacian. Fortunately, the diagonalisation of the Laplace operator on $\dsphere$ is a well-studied problem. In the following paragraph we first proof the commutativity of the operators before covering the definition of the RKHS.

\paragraph{Commutativity of $\mathcal{K}$ and $\LaplaceBeltrami$.}
We now proof that the Laplacian and the kernel operator of zonal kernels commute. For this, we first show that for zonal kernels and $x,s \in \dsphere$ the following property holds
\begin{align}
    \LaplaceBeltrami_x k(x, x') 
     &= \nabla_x \cdot \nabla_x \kappa(x\transpose x') && \text{Definition $\LaplaceBeltrami$ and zonal kernels}\\
     &= \nabla_x \cdot (x' \kappa'(x\transpose x'))  && \text{Chainrule} \\
     &= \norm{x'}^2 \kappa''(x\transpose x') = \kappa''(x \transpose x') && \text{Because } \norm{x'} = 1.
\end{align}
Similarly, for $\LaplaceBeltrami_s k(x, x')$ we obtain $k''(x \transpose x')$, and as a result
\begin{equation}
    \label{eq:LaplaceBeltrami-x-s}
\LaplaceBeltrami_x k(x, x') = \LaplaceBeltrami_{x'} k(x, x').
\end{equation}
Relying on integration by parts and the previous result, we obtain
\begin{align}
    \mathcal{K} \left[\LaplaceBeltrami f\right] &= \int_{\dsphere} k(x, x') \left[\LaplaceBeltrami_x f(x)\right] \calcd{\nu(x)} && \text{Definition $\mathcal{K}$, \cref{eq:kernel-operator-zonal}}\\
    &= \int_{\dsphere} f(x) \LaplaceBeltrami_x k(x, x')  \calcd{\nu(x)} && 2\,\times\,\text{Integration by parts}\\
    &= \int_{\dsphere} f(x) \LaplaceBeltrami_{x'} k(x, x')  \calcd{\nu(x)}  && \text{\cref{eq:LaplaceBeltrami-x-s}} \\
    &= \LaplaceBeltrami \left[ \mathcal{K} f \right]
\end{align}
which shows that the two operators commute and in turn implies that they share the same eigenfunctions. This result is of particular relevance to us since there is a huge body of literature on diagonalisation of the Laplace-Beltrami operator on $\dsphere$, and that it is well known that its eigenfunctions are given by the spherical harmonics. The spherical harmonics $\phi_{n,j}$ form an orthonormal basis for the square-integrable functions on the hypersphere. They are indexed with a level $n$ and an index within each level $j \in \{1, \dots, \dnumharmonicsforlevel\}$. See \cref{appendix:spherical-harmonics} for a comprehensive introduction to spherical harmonics, as well as a practical algorithm to compute them for relatively large $d$. This reasoning can be summarised by the following theorem:
\begin{theorem}[Mercer decomposition]
Any zonal kernel $k(x, x') = \kappa(x\transpose x')$ on the hypersphere can be decomposed as
\begin{equation}
\label{eq:kernel-form}
    k(x, x') = \sum_{n=0}^{\infty} \sum_{j=1}^{\dnumharmonicsforlevel} \lambda_{n} \phi_{n,j}(x) \phi_{n,j}(x'),
\end{equation}
where $x,x' \in \dsphere$ and $\lambda_{n}$ are positive coefficients, $\phi_{n,j}$ denote the elements of the spherical harmonic basis in $\dsphere$, and $\dnumharmonicsforlevel$ corresponds to the number of spherical harmonics for a given level $n$. As a result of the Funk-Hecke theorem, the associated eigenvalues only depending on the level $n$
\begin{equation}
    \label{eq:compute-eigenvalues}
        \lambda_{n} = \frac{\omega_{d}}{C_n^{(\alpha)}(1)} \int_{-1}^1 \kappa(t)\,C_n^{(\alpha)}(t)\,(1 - t^2)^{\frac{d-3}{2}} \calcd{t},
\end{equation} 
where $C_n^{(\alpha)}$ is the Gegenbauer polynomial of degree $n$, $\alpha = \frac{d-2}{2}$, $\omega_d$ is a constant that depends on the surface area of the hypersphere. Analytic expressions are given in \cref{appendix:spherical-harmonics}.
\end{theorem}
Although it is typically stated without a proof, this theorem is already known in some communities (see \citet{wendland2005} for a functional analysis exposition, or \citet{peacock1999cosmological} for its use in cosmology). Given the Mercer decomposition, we can equivalently, define the associated RKHS as
\begin{equation}
    \label{eq:inner-product-rkhs}
    \rkhs = \left\{
    f = 
    \sum_{n=0}^\infty \sum_{j=1}^{\dnumharmonicsforlevel} {f}_{n,j} \sh_{n, j}:
    \norm{f}_{\rkhs} < \infty
    \right\},
    \quad\text{where}\quad
    \langle g, h \rangle_{\rkhs} = 
    \sum_{n,j}
            \frac{{g}_{n, j} {h}_{n, j}}{\lambda_{n}}
\end{equation}
is the \emph{reproducing} inner product between two functions $g,h \in \rkhs$. The reproducing property of the RKHS implies that for $f \in \rkhs$: $f(x) = \langle f, k(x, \cdot) \rangle_\rkhs$, which will enable the constuction of spherical harmonic inducing features of the next section.


% The shape function can be interpreted as a kernel itself, since it is the restriction of $k(\cdot, \cdot)$ to the unit hypersphere. Furthermore its expression only depends on the dot-product between the inputs so it is a zonal kernel (also known as a dot-product kernel \citep{bietti2020deep}). This means that the eigenfunctions of the angular part of $k(\cdot, \cdot)$ are the spherical harmonics $\sh_{n, j}$ (we index them with a level $n$ and an index within each level $j \in \{1, \dots, \dnumharmonicsforlevel\}$) \citep{wendland2005,Dutordoir2020spherical}. Their associated eigenvalues only depend on $n$:
% \begin{equation}
% \label{eq:compute-fourier-coefficients}
%     \lambda_{n} = 
%    \frac{\omega_{d}}{C_n^{(\alpha)}(1)} \int_{-1}^1 s(t)\,C_n^{(\alpha)}(t)\,(1 - t^2)^{\frac{d-3}{2}} \calcd{t},
% \end{equation}
% where $C_n^{(\alpha)}(\cdot)$ is the Gegenbauer polynomial\footnote{See \cref{app:sec:harmonics} for a primer on Gegenbauer polynomials and spherical harmonics.} of degree $n$, $\alpha = \frac{d-2}{2}$, $\omega_d$ is a constant that depends on the surface area of the hypersphere. Analytical expressions of $\lambda_n$ are provided in \cref{app:sec:compute-eigenvalues}.
% The above implies that $k$ admits the Mercer representation:
% \begin{equation}
%     \label{eq:mercer}
%     k(\vx, \vx') = \norm{\vx}\, \norm{\vx'} \sum_{n=0}^\infty \sum_{j=1}^{\dnumharmonicsforlevel}  \lambda_n \sh_{n, j}\left(\frac{\vx}{\norm{\vx}}\right)\, \sh_{n, j}\left(\frac{\vx'}{\norm{\vx'}}\right),
% \end{equation}
% and that the inner product between any two functions $g,\, h \in \rkhs$ is given by:
% \begin{equation}
% \label{eq:RKHSinnerproduct}
% \big\langle g, h \big\rangle_{\rkhs} = 
%     \sum_{n,j}
%             \frac{{g}_{n, j} {h}_{n, j}}{\lambda_{n}}
% \end{equation}
% where $g_{n, j}$ and $h_{n, j}$ are the Fourier coefficients of $g$ and $h$, i.e. $g(\vx) = \sum_{n,j} g_{n,j} \norm{\vx} \sh_{n,j}(\vx)$.

\section{Variational Spherical Harmonic Gaussian Processes}

\subsection{Spherical Harmonics as Inducing Features}
\label{sec:vish}

We can now build on the results from the previous section to propose powerful and efficient sparse GPs models. We want features $\kux$ that exhibit non-local influence for expressiveness, and inducing variables that induce sparse structure in $\Kuu$ for efficiency. We achieve this by defining the inducing variables $u_m$ to be the inner product between the GP\footnote{Although $f$ does not belong to $\rkhs$ \citep{kanagawa2018gaussian}, such expression is well defined since the regularity of $\sh_{m}$ can be used to extend the domain of definition of the first argument of the inner product to a larger class of functions. See \citet{hensman2017variational} for a detailed discussion.} and spherical harmonics:\footnote{Note that in the context of inducing variables, we switch to a single integer $m$ to index the spherical harmonics and order them first by increasing level $\ell$, and then by increasing $k$ within a level.}
\begin{equation}
  u_m = \langle f, \sh_m\rangle_{\rkhs}.
\end{equation}

To leverage these new inducing variables we need to compute two quantities: 1) the covariance between $u_m$ and $f$ for $\kux$, and 2) the covariance between the inducing variables themselves for the $\Kuu$ matrix. See \citet{GPflow2020multioutput} for an in-depth discussion of these concepts.

The covariance of the inducing variables with $f$ are
\begin{equation*}
   \left[\kux \right]_m = \ExpSymb[f(\vx)\,u_m] % = \ExpSymb \left[f(x) u_m\right] 
    = \langle k(\vx, \cdot), \sh_m \rangle_{\rkhs} 
    = \sh_{m}(\vx),
\end{equation*}
where we relied on the linearity of both expectation and inner product and where we used the reproducing property of $\rkhs$.

The covariance between the inducing variables is given by
\begin{equation*}
    \left[\Kuu \right]_{m, m'} = \ExpSymb \left[u_m\,u_{m'} \right] 
    % = \ExpSymb \left[ \langle f, Y_{m} \rangle  \langle f, Y_{m'} \rangle  \right] 
    = \langle \sh_{m}, \sh_{m'} \rangle_{\rkhs} 
    = \frac{\delta_{mm'}}{\widehat{a}_m}\, ,
\end{equation*}
where $\delta_{mm'}$ is the Kronecker delta. Crucially, this means that $\Kuu$ is a diagonal matrix with elements $1/(\widehat{a}_m)$.

Substituting $\Kuu$ and $\kux$ into the sparse variational approximation (\cref{eq:qf}), leads to the following form for $q(f)$
\begin{equation*}
    \GP\left(\tilde{\bm{\Phi}}^\top(\vx) \vm;\ k(\vx, \vx') + \tilde{\bm{\Phi}}^\top(\vx) (\MS - \Kuu) \tilde{\bm{\Phi}}(\vx') \right),
\end{equation*}
with $\tilde{\bm{\Phi}}(\vx) = [\widehat{a}_m \phi_m(\vx)]_{m=1}^M$.

% This sparse approximation has two main differences compared to a SVGP model with standard inducing points. First, the spherical harmonic inducing variables lead to features $k_\vu(\vx)$ with non-local structure. Second, the approximation $q(f)$ does not require any inverses anymore. The computational bottleneck of this model is now simply the matrix multiplication in the variance calculation, which has a $\BigO(N_{\text{batchsize}} M^2)$ cost. Compared to the $\BigO(M^3 + N_{\text{batchsize}} M^2)$ cost of inducing point SVGPs, this gives a significant speedup -- as we show in the experiments.

% Finally, using v and diagonal covariance matrices $\Kuu$. Combined, this enables more accurate and more efficient GP modelling, as we will demonstrate in the next section. We can deal with regression using the Titsias bound \citep{titsias2009} in $\BigO(N M^2)$ and deal with non-conjugate likelihoods for classification using stochastic optimisation \citep{hensman2013} in $\BigO(N_{\text{batchsize}} M^2)$. 

As is usual in sparse GP methods, the number of inducing variables $M$ is constrained by the computational budget available to the user. Given that we ordered the spherical harmonic by increasing $\ell$, choosing the first $M$ elements means we will select first features with low angular frequency. Provided that the kernel spectral density is a decreasing function (this will be true for classic covariances, but not for quasi-periodic ones), this means that the selected features correspond to the ones carrying the most signal according to the prior. In other words, the decomposition of the kernel can be compared to an infinite dimensional principal component analysis, and our choice of the inducing function is optimal since we pick the ones with the largest variance. This is illustrated in \cref{fig:variance_decay_SH}, which shows the analogue of \cref{fig:variance_decay_VFF} for spherical harmonic inducing functions.

\section{Homogeneous Extension to $\Reals^d$}


\section{Experiments}

\subsection{Comparison to Variational Fourier Features}

\subsection{Toy Example on Banana}

\subsection{Airline}

