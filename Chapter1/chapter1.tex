\chapter{Introduction}  %Title of the First Chapter

Using past experience to update ones behaviour is what differentiates intelligent beings from other creatures in our world. To artificially create said systems, that can learn from data acquired through experience, is the crowning goal of the field of Artificial Intelligence (AI) and Machine Learning (ML).
%In Machine Learning, which is a subdomain of AI, this is done through the manipulation of mathematical formalisms that can be translated into computer algorithms. 
Realising this goal will have a large impact on the world and society we live in, as it will free up humans from tasks that require cognition, like driving cars, booking appointments, and interpreting and acting on medical records, to give a few examples. Though, arguably, the field has still a long way to go before fullfilling its full potential.

An elegant approach to formalise the concept of learning through acquired experience is \emph{Bayesian learning}. In this framework one specifies beliefs over quantities of interest, and updates them after observing data. The beliefs are represented using probability distributions: the \emph{prior} describes what is known about the quantity before any data is observed, and the so-called \emph{likelihood} describes the relation between the observed data and the quantity of interest. The framework provides a recipe for obtaining an updated belief over the quantity of interest after data has been observed. This distribution is known as the \emph{posterior}. Bayesian learning makes decisions atop these beliefs and uses the posterior to reason about the optimality of a decision or the cost associated to a certain action.

The performance of a decision-making system will depend on the speed at which it can learn and the optimality of the decisions made---quantified by the \emph{regret}. It can be shown that, under certain assumptions, Bayesian learning gives rise to the optimal regret. However, such excellence comes at a high computational cost, which in many scenarios renders Bayesian learning---in its purest form---unpratical. Fortunately, the literature has proposed many approximate methods which have lightened the computational complexity of the Bayesian paradigm.

Unquestionably, (approximate) models for decision-making systems need to be able to handle \emph{uncertainty}. They need to be able to quantify what is known, and what is not known. The importance of quantifying uncertainty for decision-making systems becomes clear from the many sources it can stem from. For example, there can be multiple different settings of a model that explain the data, so one needs to be uncertain about the setting that actually generated it. One also needs to be uncertain about the model itself, as most probably the model at hand is a simplification of the real-world process. Moreover, the environment in which the system operates may also be inherintly uncertain, in which case even an infinite amount of data (i.e. experience) would not make the system any smarter (e.g., trying to predict the outcome of rolling a fair dice).

In my opinion, Gaussian processes are the perfect compromise between computational complexity and Bayesian rigor. Their non-parameteric nature makes them complex enough to model a wide range of problems, while their kernel formulation makes them applicable to many different domains: graphs, vectors, images, etc. Instead of representing probability distributions on weights, Gaussian processes can be used to represent uncertainty directly on the function that the weights represents. This makes Gaussian processes, as opposed to parametric models, more amenable to mathematical analysis, which makes it possible to give strong garantuees on the future performance of the system. This may be necessary for deploying these systems in commerical applications. For these reasons, I believe, studying Gaussian processes is very worthwhile.

In this report, I present two pieces of novel research in the domain of approximate Bayesian inference for Gaussian process models. In \cref{chapter:vish}, I introduce an interdomain inducing variable approach that speeds up inference and prediction in Gaussian processes by two orders of magnitude. An important building block for this work are spherical harmonics, for which we developed an efficient algorithm for their evaluation in high dimensions. In \cref{sec:spherical-harmonics} I present this algorithm and its theoretical foundation. In \cref{chapter:dnn-as-point-estimate-for-dgps} we marry the strengths of deep neural networks and deep Gaussian processes by establishing an equivalence between the forward passes of both models. This results in models that can either be seen as neural networks with improved uncertainty prediction or deep Gaussian processes with increased prediction accuracy. The final part of this report, \cref{chapter:future-research}, is devoted to future research. Finally, we start with covering the necessary theoretical background in \cref{chapter:theoretical-framework}.

The material presented in \cref{chapter:vish,chapter:dnn-as-point-estmate-for-dgps} is published or is currently under review:
\begin{enumerate}
    \item \fullcite{Dutordoir2020spherical}
    \item \fullcite{dutordoir2021deep}
    \item \fullcite{dutordoir2021gpflux}
\end{enumerate}


% Bayesian inference has the potential to improve deep neural networks (DNNs) by providing 1) uncertainty estimates for robust prediction and downstream decision-making, and 2) an objective function (the marginal likelihood) for hyperparameter selection \citep{mackay1992practical,mackay1992bmc,mackay2003information}. The recent success of deep learning \citep{alexnet,attention,Schrittwieser2020MasteringAG} has renewed interest in large-scale Bayesian Neural Networks (BNNs) as well, with effort mainly focused on obtaining useful uncertainty estimates \citep{blundell2015,Kingma2015local,Gal2016dropout}.
% Despite already providing usable uncertainty estimates, there is significant evidence that current approximations to the uncertainty on neural network weights can still be significantly improved \citep{hron2017variational,foong2019expressiveness}. The accuracy of the uncertainty approximation is also linked to the quality of the marginal likelihood estimate \citep{blei2017variational}. Since hyperparameter learning using the marginal likelihood fails for most common approximations \citep[e.g.,][]{blundell2015}, the accuracy of the uncertainty estimates is also questionable.

% Instead of representing probability distributions on weights, Gaussian processes (GPs) can be used to represent uncertainty directly on the function that the weights represent \citep{rasmussen2006}. \citet{Damianou2013} used Gaussian processes as layers to create a different Bayesian analogue to a DNN. These Deep Gaussian Processes (DGPs) are particularly promising because Bayesian approximations seem to be of higher quality than those of weight-space BNNs, e.g.~as supported by the successful use of marginal likelihood estimates for hyperparameter learning \citep{Damianou2013,damianou2015thesis,Dutordoir2020convolutional}.

% \citet{Damianou2013} used Gaussian processes \citep{rasmussen2006} as layers to create a different Bayesian analogue to a DNN: the Deep Gaussian process (DGP). Gaussian processes (GPs) are a different representation of a single layer neural network, which is promising because it allows high-quality approximations to uncertainty \citep{titsias2009,burt2019}. DGPs are promising, since they seem to inherit these high-quality approximations, as indicated by the successful use of marginal likelihood approximations for hyperparameter learning \citep{Damianou2013,Dutordoir2020convolutional}.

%Despite their advantages, DGPs have not been adopted as widely as DNNs, which can mainly be attributed to 1) larger computational costs, and 2) slow optimisation.
% Despite their advantages, DGPs have not been adopted as widely as DNNs, which can mainly be attributed to their larger computational cost and the fact that their are challenging to optimise.
% In the past, these challenges were also present in DNNs, although decades of work has led to standard practices (e.g.~ReLU activations \citep{relu}, Xavier weight initialisations \citep{glorot2010understanding}, and batch normalisation \citep{batchnorm}) that have largely eliminated them. So far, it has not been possible to directly apply these methods to DGPs due to the mathematical differences between the models.

% \section{Option 1: Focus on what is in the report}

% \begin{enumerate}
%     \item Neural networks and Gaussian processes: complementry strengths and weaknesses
%     \item Ideally we have a single model that can handle low and high dimensional inputs, make robust uncertainty-aware predictions and can be used in big and small data regimes.
%     \item Gaussian processes and Bayesian Neural networks: connections \citep{neal1992bayesian,neal1996bayesian,williams1996gaussian}
%     \item Problem with BNN is that (approximate) Bayesian inference is challening
%     \item Deep Gaussian processes \citep{Damianou2013}
%     \item Require accurate and scalable approximate Bayesian inference procedures
% \end{enumerate}


% \section{Option 2: Focus on the direction of the future work}
% Introduction to data-driven decision making using Bayesian Machine Learning.

% \begin{enumerate}
%     \item Data-driven Decision-making
%     \item The data can be explained by many models
%     \item Models that represent uncertainty
%     \item Probabilistic machine learning is all about inferring the right model given the data -- by making use of probability theory.
%     \item Statistical learning theory: Emperical risk minimisation
%     \item No Free Lunch Theorem
%     \item Bayesian Linear Regression: $f(x) = w^\top \phi(x)$
%     \item Parametric models
%     \item Probabilistic machine learning: Bayes Rule
%     \item Kernel methods
% \end{enumerate}


% \subsection{Draft}
% As the world we live in grows ever more interconnected and complex, making good decisions becomes increasingly difficult. Heterogenous transportation systems in large cities need to be optimised, global supply chains must be operated such thay they are as efficient and reliable as possible, and connected smart agents, such as self-driving cars, need to adhere to a policy that benefits the overall cause. Humans can typically make good decisions when the number of covariates is small, but suffer quickly when they have to consider thousands of influencing factors, or have to take into account the impact of their earlier decisions on future ones. Data-driven decision-making is a general framework that studies this problem. The aim of data-driven decision-making is to use past and current information to build a model of the environment that can be used to reason about future decisions and their impact.

% A crucial building block for data-driven decision-making is a \emph{model}, which tries to explain the given data.
% uncertainty
% many explanations for the data
% aleatoric
% epistemic 


% Supervised machine learning setting
% $x \in \mathcal{X}$ and $y \in \Reals$, and a dataset $\data = \{x_i, y_i\}_{i}^N$

% % General problem:
% % \begin{equation}
% %     \argmin_{f} \sum_i L(f(x_i), y_i) + \norm{f}
% % \end{equation}



% \section{Contributions and Layout of this Report}

% This report represents my learning and the research that I conducted during the first year of my PhD degree. Most notably, we developed a novel sparse approximation for (deep) Gaussian processes based on the decomposition of the kernel in Spherical harmonics. In \cref{chapter:theoretical-framework} we cover the necessary theoretical background.

% \begin{description}
%     \item[Chapter 3] In this chapter we introduce a new class of inter-domain variational GPs where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. The inference scheme is comparable to Variational Fourier Features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. The experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy.
    
%     The content of this chapter is largely based on:

%     \fullcite{Dutordoir2020spherical},

%     with the exception of the algorithm for computing the spherical harmonics in high dimensions.

%     \item[Chapter 3] Following up on the previous chapter, we use the decomposition of zonal kernels to design an interdomain inducing variable that mimics the behaviour of activation functions is neural network layers. 

%     The content of this chapter is largely based on:

%     \fullcite{dutordoir2021deep}.

%     \item[Chapter 4] In the last chapter of the report we will shed a light on what the future work will focus on: ``Gaussian Decision Systems with Geometric Gaussian processes".
% \end{description}


% Chapter 2 provides the theoretical background 
% Chapter 3 Spherical Harmonics inducing variables
% Chapter 4 Connection between NN and DGPs
% Chapter 5 focuses on future researce