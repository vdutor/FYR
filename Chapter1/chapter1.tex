\chapter{Introduction}  %Title of the First Chapter

Bayesian inference has the potential to improve deep neural networks (DNNs) by providing 1) uncertainty estimates for robust prediction and downstream decision-making, and 2) an objective function (the marginal likelihood) for hyperparameter selection \citep{mackay1992practical,mackay1992bmc,mackay2003information}. The recent success of deep learning \citep{alexnet,attention,Schrittwieser2020MasteringAG} has renewed interest in large-scale Bayesian Neural Networks (BNNs) as well, with effort mainly focused on obtaining useful uncertainty estimates \citep{blundell2015,Kingma2015local,Gal2016dropout}.
Despite already providing usable uncertainty estimates, there is significant evidence that current approximations to the uncertainty on neural network weights can still be significantly improved \citep{hron2017variational,foong2019expressiveness}. The accuracy of the uncertainty approximation is also linked to the quality of the marginal likelihood estimate \citep{blei2017variational}. Since hyperparameter learning using the marginal likelihood fails for most common approximations \citep[e.g.,][]{blundell2015}, the accuracy of the uncertainty estimates is also questionable.

% Instead of representing probability distributions on weights, Gaussian processes (GPs) can be used to represent uncertainty directly on the function that the weights represent \citep{rasmussen2006}. \citet{Damianou2013} used Gaussian processes as layers to create a different Bayesian analogue to a DNN. These Deep Gaussian Processes (DGPs) are particularly promising because Bayesian approximations seem to be of higher quality than those of weight-space BNNs, e.g.~as supported by the successful use of marginal likelihood estimates for hyperparameter learning \citep{Damianou2013,damianou2015thesis,Dutordoir2020convolutional}.

\citet{Damianou2013} used Gaussian processes \citep{rasmussen2006} as layers to create a different Bayesian analogue to a DNN: the Deep Gaussian process (DGP). Gaussian processes (GPs) are a different representation of a single layer neural network, which is promising because it allows high-quality approximations to uncertainty \citep{titsias2009,burt2019}. DGPs are promising, since they seem to inherit these high-quality approximations, as indicated by the successful use of marginal likelihood approximations for hyperparameter learning \citep{Damianou2013,Dutordoir2020convolutional}.

%Despite their advantages, DGPs have not been adopted as widely as DNNs, which can mainly be attributed to 1) larger computational costs, and 2) slow optimisation.
Despite their advantages, DGPs have not been adopted as widely as DNNs, which can mainly be attributed to their larger computational cost and the fact that their are challenging to optimise.
In the past, these challenges were also present in DNNs, although decades of work has led to standard practices (e.g.~ReLU activations \citep{relu}, Xavier weight initialisations \citep{glorot2010understanding}, and batch normalisation \citep{batchnorm}) that have largely eliminated them. So far, it has not been possible to directly apply these methods to DGPs due to the mathematical differences between the models.

% \section{Option 1: Focus on what is in the report}

% \begin{enumerate}
%     \item Neural networks and Gaussian processes: complementry strengths and weaknesses
%     \item Ideally we have a single model that can handle low and high dimensional inputs, make robust uncertainty-aware predictions and can be used in big and small data regimes.
%     \item Gaussian processes and Bayesian Neural networks: connections \citep{neal1992bayesian,neal1996bayesian,williams1996gaussian}
%     \item Problem with BNN is that (approximate) Bayesian inference is challening
%     \item Deep Gaussian processes \citep{Damianou2013}
%     \item Require accurate and scalable approximate Bayesian inference procedures
% \end{enumerate}


% \section{Option 2: Focus on the direction of the future work}
% Introduction to data-driven decision making using Bayesian Machine Learning.

% \begin{enumerate}
%     \item Data-driven Decision-making
%     \item The data can be explained by many models
%     \item Models that represent uncertainty
%     \item Probabilistic machine learning is all about inferring the right model given the data -- by making use of probability theory.
%     \item Statistical learning theory: Emperical risk minimisation
%     \item No Free Lunch Theorem
%     \item Bayesian Linear Regression: $f(x) = w^\top \phi(x)$
%     \item Parametric models
%     \item Probabilistic machine learning: Bayes Rule
%     \item Kernel methods
% \end{enumerate}


% \subsection{Draft}
% As the world we live in grows ever more interconnected and complex, making good decisions becomes increasingly difficult. Heterogenous transportation systems in large cities need to be optimised, global supply chains must be operated such thay they are as efficient and reliable as possible, and connected smart agents, such as self-driving cars, need to adhere to a policy that benefits the overall cause. Humans can typically make good decisions when the number of covariates is small, but suffer quickly when they have to consider thousands of influencing factors, or have to take into account the impact of their earlier decisions on future ones. Data-driven decision-making is a general framework that studies this problem. The aim of data-driven decision-making is to use past and current information to build a model of the environment that can be used to reason about future decisions and their impact.

% A crucial building block for data-driven decision-making is a \emph{model}, which tries to explain the given data.
% uncertainty
% many explanations for the data
% aleatoric
% epistemic 


% Supervised machine learning setting
% $x \in \mathcal{X}$ and $y \in \Reals$, and a dataset $\data = \{x_i, y_i\}_{i}^N$

% % General problem:
% % \begin{equation}
% %     \argmin_{f} \sum_i L(f(x_i), y_i) + \norm{f}
% % \end{equation}



\section{Contributions and Layout of this Report}

This report represents my learning and the research that I conducted during the first year of my PhD degree. Most notably, we developed a novel sparse approximation for (deep) Gaussian processes based on the decomposition of the kernel in Spherical harmonics. In \cref{chapter:theoretical-framework} we cover the necessary theoretical background.

\begin{description}
    \item[Chapter 3] In this chapter we introduce a new class of inter-domain variational GPs where data is mapped onto the unit hypersphere in order to use spherical harmonic representations. The inference scheme is comparable to Variational Fourier Features, but it does not suffer from the curse of dimensionality, and leads to diagonal covariance matrices between inducing variables. This enables a speed-up in inference, because it bypasses the need to invert large covariance matrices. The experiments show that our model is able to fit a regression model for a dataset with 6 million entries two orders of magnitude faster compared to standard sparse GPs, while retaining state of the art accuracy.
    
    The content of this chapter is largely based on:

    \fullcite{Dutordoir2020spherical},

    with the exception of the algorithm for computing the spherical harmonics in high dimensions.

    \item[Chapter 3] Following up on the previous chapter, we use the decomposition of zonal kernels to design an interdomain inducing variable that mimics the behaviour of activation functions is neural network layers. 

    The content of this chapter is largely based on:

    \fullcite{dutordoir2021deep}.

    \item[Chapter 4] In the last chapter of the report we will shed a light on what the future work will focus on: ``Gaussian Decision Systems with Geometric Gaussian processes".
\end{description}
% Chapter 2 provides the theoretical background 
% Chapter 3 Spherical Harmonics inducing variables
% Chapter 4 Connection between NN and DGPs
% Chapter 5 focuses on future researce