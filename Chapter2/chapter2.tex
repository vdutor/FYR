%!TEX root = ../thesis.tex

\chapter{Theoretical Framework}
\label{chapter:theoretical-framework}

This chapter discusses Gaussian processes (GP) and deep Gaussian processes (DGPs), the composite model obtained by stacking multiple GP models on top of each other. We also review how to perform approximate Bayesian inference in these models, with a particular attention to Variational Inference. We also cover the theory of postive definite kernels and the Reproducing Kernel Hilbert Spaces (RKHS) they characterise.

\section{Gaussian Processes}
\label{sec:chapter1:gp}


Gaussian processes (GPs) \citep{rasmussen2006} are non-parametric distributions over functions similar to Bayesian Neural Networks (BNNs). The core difference is that neural networks represent distributions over functions through distributions on weights, while a Gaussian process specifies a distribution on function values at a collection of input locations. This representation allows us to use an infinite number of basis functions, while still enables Bayesian inference \citep{neal1996bayesian}. 

Following from the Kolmogorov extension theorem, we can construct a real-valued stochastic process (i.e. function) on a non-empty set $\mathcal{X}$, $f: \mathcal{X} \rightarrow \Reals$, if there exists on all finite subsets $\{x_1, \ldots x_N\} \subset \mathcal{X}$, a \emph{consistent} collection of finite-dimensional marginal distributions over $f(\{x_1, \ldots, x_n\})$. For a Gaussian process, in particular, the marginal distribution over every finite-dimensional subset is given by a multivariate normal distribution. This implies that, in order to fully specify a Gaussian process, it suffice to only define the mean and covariance (kernel) function because they are the sufficient statistics for every finite-dimensional marginal distribution. We can therefore denote the GP as
\begin{equation}
  \label{eq:chapter1:def-gp}
  f \sim \GP(\mu, k),
\end{equation}
where $\mu:\mathcal{X}\rightarrow\Reals$ is the mean function, which encodes the expected value of $f$ at every $x$, $\mu(x) = \Exp{f}{f(x)}$, and $k:\mathcal{X} \times \mathcal{X} \rightarrow \Reals$ is the covariance (kernel) function that describes the covariance between function values, $k(x, x') = \Cov\left(f(x), f(x')\right)$. The covariance function has to be a symmetric, positive-definite function. The Gaussianity, and the fact that we can manipulate function values at some finite points of interest without taking the behaviour at any other points into account (the marginalisation property) make GPs particularly convenient to manipulate and use as priors over functions in Bayesian models ---as we will show next.

Througout this report, we will consider $f$ to be the complete function, and intuitively manipulate it as an infinitely long vector. When necessary, we will append `$(\cdot)$' to $f$ to highlight that it is indeed a function rather than a finite vector. Contrarily, $f(\vx) \in \Reals^N$ denotes the function evaluated at a finite set of points $\vx = \{x_1, \cdots, x_N\}$. $f^{\setminus \vx}$ denotes another infinitely long vector similar to $f$ but excluding $f(\vx)$. Intuitively, $f$ can be partioned into two groups: $f(\vx)$ and $f^{\setminus \vx}$, with the following joint distribution
% Joint
\begin{equation}
  \begin{pmatrix} f^{\setminus \vx}(\cdot)\\ f(\vx) \end{pmatrix} \sim \NormDist{
    \begin{pmatrix} \mu(\cdot) \\ \vmu_{\vf} \end{pmatrix},
    \begin{pmatrix} k(\cdot, \cdot)  & \kfx\transpose \\ \kfx & \Kff \end{pmatrix},
  }
\end{equation}
where $[\vmu_\vf]_{i} = \mu(x_i)$, $[\Kff]_{i,j} = k(x_i, x_j)$, $[\kfx]_i = k(x_i, \cdot)$, and $\mu(\cdot)$ and $k(\cdot, \cdot)$ are the mean and covariance function from \cref{eq:chapter1:def-gp}. Following the marginalisation property, the distribution over $f(\vx)$ is given by
\begin{equation}
p(f(\vx)) = \int p(f)\,\calcd{f^{\setminus \vx}} = \NormDist{\vmu_{\vf}, \Kff}.
\end{equation}
% Condition
We can now specify the GP at this finite set of points and use the rules of conditioning to obtain the GP elsewhere. Let $f(\vx) \shorteq \vf$, then the conditional distribution for $f^{\setminus \vx}$ is given by another Gaussian process
\begin{equation}
  \label{eq:chapter1:conditional}
  p(f^{\setminus \vx}(\cdot) \given f(\vx) \shorteq \vf) = \GP\Big(\mu(\cdot) + \kfx^\top \Kff^{-1} (\vf - \vmu_{\vf}),\ \ k(\cdot, \cdot) -  \kfx^\top \Kff^{-1} \kfx\Big),
\end{equation}
The conditional distribution over the whole function $p(f(\cdot) \given f(\vx) = \vf)$ has the exact same form as in \cref{eq:chapter1:conditional}. This is mathematically slightly confusing because the random variable $f(\vx)$ is included both on the left and right-hand-side of the conditioning, but the equation is technically correct, as discussed in \citet{matthews16,van2020framework,Leibfried2020Tutorial}.

\subsection{The Beauty of Gaussian Process Regression: Exact Bayesian Inference}

A defining advantages of GPs is that we can perform exact Bayesian inference in the case of regression. % In this section we briefly discuss the general methodology of Bayesian modelling and how this is performed by GPs.
% \paragraph{Problem Defintion} 
Assume a supervised learning setting where $x \in \mathcal{X}$ (typically, $\mathcal{X} = \Reals^d$) and $y \in \Reals$, and we are given a dataset $\data = \{(x_i, y_i)\}_{i=1}^N$ of input and corresponding output pairs. For convenience, we sometimes group the inputs in $\vx = \{x_i\}_{i=1}^N$ into a single design matrix and outputs $\vy = \{y_i\}_{i=1}^N$ into a vector. We further assume that the data is generated by an unknown function $f: \mathcal{X} \rightarrow \Reals$, and that the outputs are perturbed versions of functions evaluations at the corresponding inputs: $y_i = f(x_i) + \epsilon_i$. In the case of regression we assume a Gaussian noise model $\epsilon_i \sim \NormDist{0, \sigma^2}$. We are interested in learning the function $f$ that generated the data. % We denote the evaluation of the function at all inputs as $f(\vx) \in \Reals^N$ and at a single input as $f(x_i) \in \Reals$.

% The key idea in Bayesian modelling is to specify a prior distribution over the quantity of interest. The prior encodes what we know at that point in time about the quantity. In general term, this can be a lot or a little. We encode this information in the form of a distribution. Then, as more data becomes available, we use the rules of probability, an in particlar Bayes' rule, to update our prior beliefs and compute a posterior distribution (see \citet{mackay2003information, bisschop} for a thorough introduction).

Following the Bayesian approach, we specify a \emph{prior} over the parameters of interests, which in the case of GPs is the function itself. The prior is important because it characterises the search space over possible solutions for $f$. Through the prior, we can encode strong assumptions, such as linearity, differentiability, periodicity, etc. or any combination thereof. These strong inductive biases make it possible to generalise well from very limited data. Compared to Bayesian parametric models, it is much more convenient and intuitive to specify priors directly in \emph{function-space}, rather than on the weights of a parametric model \citep{rasmussen2006}. 

In Gaussian process regression (GPR), we specify a GP prior over $f$, for which we assume without loss of generality a zero mean function:
\begin{equation}
  f \sim \GP\big(0, k\big),
\end{equation}
The likelihood, describing the relation between the quantity of interest $f$ and the observed data, is given by 
\begin{equation}
  \label{eq:likelihood}
  p(\vy \given f) = \prod_{i=1}^N p(y_i \given f) = \prod_{i=1}^N \NormDist{y_i \given f(x_i), \sigma^2},
\end{equation}
where we see that, conditioned on the GP, the likelihood factorises over datapoints. The posterior over the complete function $f$ is another GP, which can be obtained using Bayes' rule:
\begin{equation}
  \label{eq:f-given-y}
  p(f \given \vy) 
      = \GP\Big(\kfx^\top (\Kff + \sigma^2 \Eye)^{-1} \vy,\ \ k(\cdot, \cdot) -  \kfx^\top (\Kff + \sigma^2 \Eye)^{-1} \kfx\Big).
\end{equation}
Similarly, the marginal likelihood (model evidence) is also available in closed-form and obtained by marginalising over the prior:
\begin{equation}
  \label{eq:log-marginal-likelihood}
  p(\vy) = \NormDist{\vy \given \vzero, \Kff + \sigma^2 \Eye}.
\end{equation}
The availablility of the posterior and the marginal likelihood in analytic form makes GPs a unique tool for inferring unknown functions from data. The marginal 

[TODO: occam's razor [Rasmussen and Ghahramani, 2001]

Despite the convenience of the analytic expressions, computing this exact posterior requires inverting the $N \times N$ matrix $\Kff$, which has a $\BigO(N^3)$ computational complexity and a $\BigO(N^2)$ memory footprint. Interestingly, the computational intractibility in GPR models originates from the prior and \emph{not} the complexity of marginalisation as is often the case for other Bayesian methods [CITE]. Indeed, these methods resort to MCMC to avoid computing intractable normalising constant, which is a problem that GPR does not encounter.  Another shortcoming of GPR models is that there is no known analytical expression for the posterior distribution when the likelihood (\cref{eq:likelihood}) is not Gaussian, as encountered in classification for instance. In the next paragraph we discuss solutions to both problems.

\section{Approximate Inference with Sparse Gaussian Processes}

Sparse Gaussian processes combined with Variational Inference (VI) provide an elegant way to address these two shortcomings \citep{titsias2009, hensman2013, hensman2015scalable}. VI consists of introducing a distribution $q(f)$ that depends on some parameters, and finding the values of these parameters such that $q(f)$ gives the best possible approximation of the exact posterior $p(f \given \vy )$. It is worth noting that there exists other approaches in the literature that adress the same issues. In particular, Expectation Propagation (EP) \citep{minka2001expectation,bui2017unifying} formulates, similar to VI, an approximation to the posterior but uses a different objective to optimise the approximation. Tangently, other sparse GP methods (e.g., FITC) \citep{Snelson05,quinonero2005unifying} approximate the model rather than the posterior. A downside of this is that the posteriors lose their non-parametric nature, which can be detrimental for the uncertainty estimates \citep{bauer2016understanding}. In what follows we will focus on Sparse Gaussian processes with variational inference because of its general applicability and overall robust performance.

We first discuss the objective used in general variational inference before specifying our particular choice for $q(f)$. Let us therefore define $q(f)$ as the approximate to the posterior, then the idea is to optimise $q(f)$ so that the distance measured by the Kullbackâ€“Leibler (KL) divergence from $q(f)$ to $p(f \given \vy)$ is minimal. Rewriting $p(f \given \vy)$ using Bayes' rule, we obtain:
\begin{equation}
  \KL{q(f)}{p(f \given \vy)} = -{\Exp{q(f)}{\log \frac{p(\vy \given f) p(f)}{q(f)}}} + \log p(\vy).
\end{equation}
Rearranging these terms yields:
\begin{equation}
  \label{eq:elbo}
  \log p(\vy) - \KL{q(f)}{p(f \given \vy)} = 
  \underbrace{\Exp{q(f)}{\log p(\vy \given f)} - \KL{q(f)}{p(f)}}_{\triangleq\,\textrm{ELBO}}
\end{equation}
The r.h.s. of \cref{eq:elbo} is known as the Evidence Lower BOund (ELBO). It is a lower bound to the log marginal likelihood ($\log p(\vy)$) because the KL between $q(f)$ and $p(\vy \given f)$ is always non-negative. Further, since $p(\vy)$ does not depend on the variational approximation, maximising the ELBO w.r.t. $q(f)$ is equivalent to minimsing the $\KL{q(f)}{p(f \given \vy)}$. The maximum will be reached when $q(f) \shorteq p(f \given \vy)$, in which case the KL will be zero and the ELBO equals the log evidence. 

Intuitively, VI casts the problem of Bayesian inference, namely marginalisation, as an optimisation problem. The objective for the optimisation problem is the ELBO, which depends on the variational approximation, the prior and the data but, importantly, not the true posterior. This approach has several advantages. Firstly, optimisation, as opposed to marginalisation, is garantueed to converge ---albeit possibly to a local optima. Secondly, VI provides a framework in which one can trade computational cost for accuracy: by expanding the family of approximating distributions we can only tighten the bound. An interesting example of this is importance weighting, where in the limit of infinite compute the true posterior is part of the approximating family \citep{Domke2018IWVI}. Finally, the bound and its derivates can be computed with stochastic estimations using the reparametrisation trick or score function estimators which enables big data settings.

So far we have discussed the bound in VI, but have left $q(f)$ unspecified, we now focus our attention to this. In general, we want the family of variational distributions to be flexible enough, such that some setting of the parameters can can approximate the true posterior well, while also being computationally efficient and mathematically convenient to manipulate. % Mark van der Wilk discusses in his thesis \citep{vdw2019thesis} also the importance of  maintaining the non-parametric uncertainty in the approximation. 
We follow the approach proposed by \citet{titsias2009}, which parameterises the approximation using a set of $M$ pseudo inputs $\vz = \{z_1, \ldots, z_M\} \in \Reals^{M \times d}$ corresponding to $M$ inducing variables $\vu = f(\vz) \in \Reals^M$. 
We choose to factorise the variational approximation as $q(f, \vu) = q(\vu) p(f\given f(\vz) \shorteq \vu)$, where
\begin{equation}
  p(f \given f(\vz) \shorteq \vu) = \GP\Big(\kux^\top \Kuu^{-1}\vu,\ \ k(\cdot, \cdot) -  \kux^\top \Kuu^{-1} \kux\Big), 
\end{equation}
where $[\Kuu]_{i,j} = k(z_i, z_j)$, $[\kux]_i = k(z_i, \cdot)$. We specify a Gaussian distribution for the variational posterior over the inducing variables with a freely parameterised mean and covariance
\begin{equation}
  q(\vu) = \NormDist{\vm, \MS}
\end{equation}
such that the overall approximate posterior is given by another GP
\begin{equation}
  q(f) = \int_{\vu} q(f, \vu)\,\calcd{\vu} = \GP\Big(\kux^\top \Kuu^{-1}\vm,\ \ k(\cdot, \cdot) -  \kux^\top \Kuu^{-1} (\Kuu + \MS)\Kuu^{-1} \kux\Big).
\end{equation}
as a result of the conjugacy of the variational posterior $q(\vu)$ and the conditional. 
The approximation admits several interesting properties. Firstly, in the case of a Gaussian likelihood and a number of inducung points that equals the amount of data, $q(f)$ contains the true posterior. That is, if $\vz \shorteq \vx$, it is possible to find $\vm$ and $\MS$ such that $q(f) = p(f \given \vy)$ \citep{titsias2009}. Secondly, by approximating the posterior, rather than the model, we obtain a non-parametric approximation which makes predictions with an infinite amount of basis functions, such as the true posterior. For non-degenarate kernels this leads to error bars that are unconstrained by the data. Finally, \citet{burt2019} showed that for $N \rightarrow \infty$, the approximate posterior can be made arbitrarly close to the true posterior for $M = \BigO(\log N)$.

We optimise the variational parameters $\{\vm, \MS\}$ by maximising the ELBO \cref{eq:elbo}. Assuming a general likelihood of the form $p(\vy \given f) = \prod_i p(y_i \given f(x_i))$ the ELBO objective can be written as
\begin{equation}
  \label{eq:elbo-gp} 
  \textrm{ELBO} = \sum_{i=1}^N \Exp{q(f(x_i))}{\log p(y_i \given f(x_i))} - \KL{q(\vu)}{p(\vu)} \,. 
\end{equation}
Crucially, the original KL between the two infinite-dimensional processes $\KL{q(f)}{p(f)}$ is mathematically equivalent to the finite-dimensional KL between the variational posterior and prior over the inducing variables. We refer the interested reader to \citet{matthews16} for a theoretical analysis of the equivalence and to \citet[Section 4.1]{Leibfried2020Tutorial} for a more intuitive explanation. The objective in \cref{eq:elbo-gp}, introduced in \citet{hensman2013,hensman2015scalable} reduces the computational complexity to $\BigO(M^2 N + M^3)$. It also allows for stochastic estimation through minibatching \citep{hensman2013} and for non-Gaussian likelihoods through Gauss-Hermite quadrature of the one-dimensinoal expectation over $q(f(x_i))$ \citep{hensman2015scalable}.

\section{Interdomain Inducing Variables}

The basis functions used in the approximate posterior mean TODO ref are determined by the covariance between the inducing variables and other function evaluations $\left[c_u(\cdot)\right]_m = \Cov(f(\cdot), u_m)$. Commonly, the inducing variables are taken to be function values $u_{m} = f(w_m)$, which leads to the kernel becoming the basis function $[c_u(\cdot)]_{m} = k(\vw_m, \cdot)$. \emph{Interdomain} inducing variables \citep{lazaro2009inter} select different properties of the GP (e.g.~integral transforms $u_m = \int f(\vx) g_m(\vx)\calcd{\vx}$), which modifies this covariance (see \citep{van2020framework,Leibfried2020Tutorial} for an overview), and therefore gives control over the basis functions.
Most current interdomain methods are designed to improve computational properties \citep{hensman2017variational,burt2020variational,Dutordoir2020spherical}. Our aim is to control $c_u(\cdot)$ to be a typical NN actvation function like a ReLU or Softplus. This was also investigated by \citet{sun2021neural}, although they found less common saturating activation functions.

\subsection{Example: heavyside inducing variable}

$f \sim \GP$ defined on $\sphere^1$ (the unit circle), $f: [-\pi, \pi] \rightarrow \Reals, \theta \mapsto f(\theta)$.

kernel (Arc Cosine order 0):

\begin{equation}
  k(\theta, \theta') = \kappa(\rho) = \pi - |\theta - \theta'|
\end{equation}

\begin{equation}
  \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} k(\theta, \theta') = 
\end{equation}

\begin{equation}
  u_m = \mathcal{L}_m(f)
\end{equation}

\begin{equation}
  \mathcal{L}_m = \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} + \int \calcd{\theta}
\end{equation}

\begin{align}
  \Cov(u_m, f(\theta')) &= \Exp{f}{\mathcal{L}_m(f)\,f(\theta')} \\
                      &=  \mathcal{L}_m k(\theta', \cdot) \\
                      &= \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} k(\theta', \theta) + \int_{-\pi}^\pi k(\theta', \theta) \calcd{\theta}
\end{align}



\section{Deep Gaussian Processes}

% \subsection{GPflux -- a library for deep Gaussian processes}
% \fullcite{dutordoir2021gpflux}


\section{Kernels and Reproducing Kernel Hilbert Space}

Kernel have a rich history in machine learning and functional analysis. Kernels became widely noticed  by the ML community when linear SVMs [CITE] were kernelised. This means that rather than using $x\top x'$, one uses a function $k(x, x')$, which corresonponds to an inner-product in some space $\phi(x)\top \phi(x')$. This concept is also referred to as the ``kernel trick'' in the litarture. This made SVMs a compelling model for non-linear modelling problems. Prior to the deep learning revolution that began in the late nillies, kernel SVMs sat the state of the art results in many tasks: XXX. Kernels are also used in splines, Priniciple Component Analysis (PCA), Gaussian processes and in general allow for the developement of versatile algorithms to analyse and process data. Kernels operate through pairwise comparisons on datapoints and are versitale because they do not impose strong assumptions regarding the type of data, such as vectors, graphs or strings, they operate on. In what follows, we are going to focus on the theoretical properties of Mercer kernels with the application of Gaussian processes in mind, and the infinite vector space these kernels characterise. This background material will be important for the remaining chapters of the thesis. The inspiration for this section came from course Julian Marial and review paper on XXX.

\begin{definition}
A positive definite (p.d.) kernel (or Mercer kernel [TODO]) on a set $\mathcal{X}$ is a function $k: \mathcal{X} \times \mathcal{X} \rightarrow \Reals$ that is
\begin{enumerate}
  \item symmetric $k(x, x') = k(x', x)$, and
  \item satisfies for all $x_i$, $x_j \in \mathcal{X}$ and $c \in \Reals$: $\sum_{i}\sum_j c_i c_j k(x_i, x_j)\ge 0$.
\end{enumerate}
\end{definition}
One of the simplest examples of a p.d. kernel is $k(x, x') = xx'$, where $\mathcal{X} = \Reals$. This kernel is often called the linear kernel. While being simple, as we will see through the next few theorems, the associated space of functions induced by this kernel is often too restricte
In the next theorems we will see how kernels characterise a space over functions. Some of these spaces will be very explicit. For example, we will show how the linear kernel leads to a space of functions that are also linear, i.e. of the form $x \mapsto a x + b$. Other spaces belonging to kernel will be defined in a more abstract way. 

\begin{theorem}[Aronszjan, 1950 TODO]
  \label{theorem:k-and-H}
  The kernel $k$ is a p.d. kernel on $\mathcal{X}$ i.f.f. there exists a Hilbert space $\rkhs$ and a mapping $\phi: \mathcal{X} \rightarrow \rkhs$ such that $\forall x,x' \in \mathcal{X}$:
  \begin{equation}
    k(x, x') = \langle \phi(x), \phi(x') \rangle_{\rkhs}
  \end{equation}
\end{theorem}

We want to remind the reader that a Hilbert space $\rkhs$ is a (possibly infinite) vector space with an inner product $\langle f, g \rangle_{\rkhs}$ and norm $\norm{f} = \langle f, f \rangle_{\rkhs}^{\frac{1}{2}}$, and where any Cauchy sequence in $\rkhs$ converges in $\rkhs$. A Cauchy sequence is a sequence whose elements become arbitrarly close as the sequence progresses. Intuitively, a Hilbert space can be seen as a generalisation of an Euclidian space that is infinite dimensional. In the machine learning literature, $\rkhs$ is also commonly referred to as the feature space and $\phi$ the feature map.

A Reproducing Kernel Hilbert Space (RKHS) is a special type of Hilbert space mentioned in \cref{theorem:k-and-H}. It is a space over function from $\mathcal{X}$ to $\Reals$ that are ``parameterised'' by an element in $\mathcal{X}$. In other words, a datapoint $x \in \mathcal{X}$ is mapped to a function in the RKHS: $\phi(x) \in \rkhs$. To make it more explicit we also write $\phi(x) = k_x$ to highlight that $k_x$ is another function in the space $\mathcal{X} \rightarrow \Reals$.

\begin{definition}
  Let $\mathcal{X}$ be a set and $\rkhs$ a Hilbert space with inner-product $\langle \cdot, \cdot \rangle_{\rkhs}$. The function $k: \mathcal{X} \times \mathcal{X} \rightarrow \Reals$ is called a reproducing kernel of $\rkhs$ if
  \begin{enumerate}
    \item $\rkhs$ contains all functions of the form 
    \item 
  \end{enumerate}
\end{definition}

\begin{enumerate}
  \item History: RKHS pure math and into machine learning for SVM, splines and 
  \item Positive Definite and Symmetry
  \item RKHS
  \item Bochner's theorem
  \item Mercer Decomposition
  \item Examples of RKHS
  \item RKHS through Spectral Decomposition
  \item Representer Theorem
  \item Show how sparse approximation links anchor points
\end{enumerate}

\section{Spectral Representation of Gaussian Processes}
