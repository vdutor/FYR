%!TEX root = ../thesis.tex

\chapter{Theoretical Framework}
\label{chapter:theoretical-framework}

This chapter discusses Gaussian processes (GP) and deep Gaussian processes (DGPs), the composite model obtained by stacking multiple GP models on top of each other. We also review how to perform approximate Bayesian inference in these models, with a particular attention to Variational Inference. We also cover the theory of postive definite kernels and the Reproducing Kernel Hilbert Spaces (RKHS) they characterise.

\section{Gaussian Processes}
\label{sec:chapter1:gp}


Gaussian processes (GPs) \citep{rasmussen2006} are non-parametric distributions over functions similar to Bayesian Neural Networks (BNNs). The core difference is that neural networks represent distributions over functions through distributions on weights, while a Gaussian process specifies a distribution on function values at a collection of input locations. This representation allows us to use an infinite number of basis functions, while still enables Bayesian inference \citep{neal1996bayesian}. 

Following from the Kolmogorov extension theorem, we can construct a real-valued stochastic process (i.e. function) on a non-empty set $\mathcal{X}$, $f: \mathcal{X} \rightarrow \Reals$, if there exists on all finite subsets $\{x_1, \ldots x_N\} \subset \mathcal{X}$, a \emph{consistent} collection of finite-dimensional marginal distributions over $f(\{x_1, \ldots, x_n\})$. For a Gaussian process, in particular, the marginal distribution over every finite-dimensional subset is given by a multivariate normal distribution. This implies that, in order to fully specify a Gaussian process, it suffice to only define the mean and covariance (kernel) function because they are the sufficient statistics for every finite-dimensional marginal distribution. We can therefore denote the GP as
\begin{equation}
  \label{eq:chapter1:def-gp}
  f \sim \GP(\mu, k),
\end{equation}
where $\mu:\mathcal{X}\rightarrow\Reals$ is the mean function, which encodes the expected value of $f$ at every $x$, $\mu(x) = \Exp{f}{f(x)}$, and $k:\mathcal{X} \times \mathcal{X} \rightarrow \Reals$ is the covariance (kernel) function that describes the covariance between function values, $k(x, x') = \Cov\left(f(x), f(x')\right)$. The covariance function has to be a symmetric, positive-definite function. The Gaussianity, and the fact that we can manipulate function values at some finite points of interest without taking the behaviour at any other points into account (the marginalisation property) make GPs particularly convenient to manipulate and use as priors over functions in Bayesian models -- as we will show next.

Throughout this report, we will consider $f$ to be the complete function, and intuitively manipulate it as an infinitely long vector. Moreover, $f(\vx) \in \Reals^N$ denotes the function evaluated at a finite set of points, whereas $f^{\setminus \vx}$ denotes another infinitely long vector similar to $f$ but excluding $f(\vx)$. From the marginalisation property it follows that integrating out over the infinitely many points that are not included in $\vx$, we obtain a valid finite-dimensional density for $f(\vx)$
\begin{equation}
p(f(\vx)) = \int p(f)\,\calcd{f^{\setminus \vx}}.
\end{equation}
In the case of GPs, this finite-dimensional marginal is given by a multivariate Gaussian distribition, fully characterised by the mean $\mu$ and the covariance function $k$
\begin{equation} 
  p(f(\vx)) = \NormDist{\vmu_{\vf}, \Kff},\quad\text{where}\quad [\vmu_\vf]_{i} = \mu(x_i)\ \text{and}\ [\Kff]_{i,j} = k(x_i, x_j).
\end{equation}
Conditioning the GP at this finite set of points leads to a conditional distribution for $f^{\setminus \vx}$, which is given by another Gaussian process
\begin{equation}
  \label{eq:chapter1:conditional}
  p(f^{\setminus \vx} \given f(\vx) = \vf) = \GP\big(\kfx^\top \Kff^{-1} (\vf - \vmu_{\vf}),\ \ k(\cdot, \cdot) -  \kfx^\top \Kff^{-1} \kfx\big),
\end{equation}
where $[\kfx]_i = k(x_i, \cdot)$. The conditional distribution over the whole function $p(f \given f(\vx) = \vf)$ has the exact same form as in \cref{eq:chapter1:conditional}. This is mathematically slightly confusing because the random variable $f(\vx)$ is included both on the left and right-hand-side of the conditioning, but the equation is technically correct \citep{matthews16}.

\subsection{The Beauty of Gaussian Process Regression: Exact Bayesian Inference}

One of the key advantages of Gaussian processes for regression is that we can perform exact Bayesian inference. % In this section we briefly discuss the general methodology of Bayesian modelling and how this is performed by GPs.
% \paragraph{Problem Defintion} 
Assume a supervised learning setting where $x \in \mathcal{X}$ (typically, $\mathcal{X} = \Reals^d$) and $y \in \Reals$, and we are given a dataset $\data = \{(x_i, y_i)\}_{i=1}^N$ of input and corresponding output pairs. For convenience, we sometimes group the inputs in $\vx = \{x_i\}_{i=1}^N$ into a single design matrix and outputs $\vy = \{y_i\}_{i=1}^N$ into a vector. We further assume that the data is generated by an unknown function $f: \mathcal{X} \rightarrow \Reals$, such that the outputs are perturbed versions of functions evaluations at the corresponding inputs: $y_i = f(x_i) + \epsilon_i$. In the case of regression we assume a Gaussian noise model $\epsilon_i \sim \NormDist{0, \sigma^2}$. We are interested in learning the function $f$ that generated the data. % We denote the evaluation of the function at all inputs as $f(\vx) \in \Reals^N$ and at a single input as $f(x_i) \in \Reals$.

% \paragraph{Bayesian Modelling}
[General introduction to Bayesian modelling]
The key idea in Bayesian modelling is to specify a prior distribution over the quantity of interest. The prior encodes what we know at that point in time about the quantity. In general term, this can be a lot or a little. We encode this information in the form of a distribution. Then, as more data becomes available, we use the rules of probability, an in particlar Bayes' rule, to update our prior beliefs and compute a posterior distribution (see \citet{mackay2003information, bisschop} for a thorough introduction).

Following the Bayesian approach, we specify a \emph{prior} over the parameters of interests, which in the case of GPs is the function itself. The prior is important because it characterises the search space over possible solutions for $f$. Through the prior, we can encode strong assumptions, such as linearity, differentiability, periodicity, etc. or any combination thereof, which makes it possible to generalise well from very limited data. Compared to (Bayesian) parametric models, it is much more convenient and intuitive to specify priors directly in \emph{function-space}, rather than on the weights of a parametric model \citep{rasmussen2006}. 

Following \cref{eq:chapter1:def-gp} the prior over function evaluations at the datapoints is defined by the covariance function $k$. As we assume a \`a-priori zero mean function $\mu$ (without loss of generality) this can be written as:
\begin{equation} 
  p(f(\vx)) = \NormDist{\vzero, \Kff},\quad\text{where}\quad [\Kff]_{i,j} = k(x_i, x_j).
  % \vmu_{\vf} \in \Reals^N\text{ and }\MK_{\vf\vf} \in \Reals^{N \times N},
\end{equation}
Given the function $f$ the likelihood factorises over datapoints and is given by a Gaussian:
\begin{equation}
  p(\vy \given f) = \prod_{i=1}^N p(y_i \given f) = \prod_{i=1}^N \NormDist{y_i \given f(x_i), \sigma^2}
\end{equation}
We can obtain the posterior over the function using Bayes' rule and the marginalisation property
\begin{align}
  p(f \given \vy) 
      &= \frac{p(f)\,p(\vy \given f)}{p(\vy)} \\
      &= p(f^{\setminus \vx} \given f(\vx)) \frac{ p(f(\vx)) \prod_{i=1}^N \NormDist{y_i \given f(x_i), \sigma^2}}{p(\vy)} \\
      &= \GP\big(\kfx^\top \Kff^{-1} \vf,\ \ k(\cdot, \cdot) -  \kfx^\top \Kff^{-1} \kfx\big), 
\end{align}
The marginal likelihood (model evidence)
\begin{equation}
  p(\vy) = \NormDist{\vy \given \vzero, \Kff + \sigma^2 \Eye}
\end{equation}

\begin{enumerate}
  \item Plot: Prior, Data, Posterior
  \item occam's razor
\end{enumerate}

\paragraph{Problems}
A common criticism for GPs is that any modification to this approach breaks the Gaussian assumption. 
\begin{enumerate}
  \item Non-Gaussian likelihoods
  \item Large datasets
  \item Transformations: log or square transform
\end{enumerate}

\paragraph{Solutions}
\begin{enumerate}
  \item Laplace
  \item Expectation Propagation
  \item Sparse Variational Inference
\end{enumerate}


\section{Approximate Inference with Sparse Gaussian Processes}

\begin{enumerate}
  \item General introduction to Variational inference \citep{blei2017variational}
variational inference (VI), where the problem of Bayesian inference is cast as an optimization problemâ€”namely, to maximize a lower bound of the logarithm of the marginal likelihood.
  \item Sparse approximations \citep{Snelson05,quinonero2005unifying}
\end{enumerate}

\section{Interdomain Inducing Variables}

The basis functions used in the approximate posterior mean TODO ref are determined by the covariance between the inducing variables and other function evaluations $\left[c_u(\cdot)\right]_m = \Cov(f(\cdot), u_m)$. Commonly, the inducing variables are taken to be function values $u_{m} = f(w_m)$, which leads to the kernel becoming the basis function $[c_u(\cdot)]_{m} = k(\vw_m, \cdot)$. \emph{Interdomain} inducing variables \citep{lazaro2009inter} select different properties of the GP (e.g.~integral transforms $u_m = \int f(\vx) g_m(\vx)\calcd{\vx}$), which modifies this covariance (see \citep{van2020framework,Leibfried2020Tutorial} for an overview), and therefore gives control over the basis functions.
Most current interdomain methods are designed to improve computational properties \citep{hensman2017variational,burt2020variational,Dutordoir2020spherical}. Our aim is to control $c_u(\cdot)$ to be a typical NN actvation function like a ReLU or Softplus. This was also investigated by \citet{sun2021neural}, although they found less common saturating activation functions.

\subsection{Example: heavyside inducing variable}

$f \sim \GP$ defined on $\sphere^1$ (the unit circle), $f: [-\pi, \pi] \rightarrow \Reals, \theta \mapsto f(\theta)$.

kernel (Arc Cosine order 0):

\begin{equation}
  k(\theta, \theta') = \kappa(\rho) = \pi - |\theta - \theta'|
\end{equation}

\begin{equation}
  \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} k(\theta, \theta') = 
\end{equation}

\begin{equation}
  u_m = \mathcal{L}_m(f)
\end{equation}

\begin{equation}
  \mathcal{L}_m = \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} + \int \calcd{\theta}
\end{equation}

\begin{align}
  \Cov(u_m, f(\theta')) &= \Exp{f}{\mathcal{L}_m(f)\,f(\theta')} \\
                      &=  \mathcal{L}_m k(\theta', \cdot) \\
                      &= \frac{\calcd{}}{\calcd{\theta}}\Bigr|_{\theta = \theta_m} k(\theta', \theta) + \int_{-\pi}^\pi k(\theta', \theta) \calcd{\theta}
\end{align}



\section{Deep Gaussian Processes}

% \subsection{GPflux -- a library for deep Gaussian processes}
\fullcite{dutordoir2021gpflux}


\section{Kernels and Reproducing Kernel Hilbert Space}

Kernel have a rich history in machine learning and functional analysis. Kernels became widely noticed  by the ML community when linear SVMs [CITE] were kernelised. This made SVMs a compelling model for non-linear modelling problems. Prior to the deep learning revolution that began in the late nillies, kernel SVMs sat the state of the art results in many tasks: XXX. Kernels are also used in splines, Priniciple Component Analysis (PCA), Gaussian processes and in general allow for the developement of versatile algorithms to analyse and process data. Kernels operate through pairwise comparisons on datapoints and are versitale because they do not impose strong assumptions regarding the type of data, such as vectors, graphs or strings, they operate on. In what follows, we are going to focus on the theoretical properties of Mercer kernels with the application of Gaussian processes in mind, and the infinite vector space these kernels characterise. This background material will be important for the remaining chapters of the thesis. The inspiration for this section came from course Julian Marial and review paper on XXX.

\begin{definition}
A positive definite (p.d.) kernel (or Mercer kernel [TODO]) on a set $\mathcal{X}$ is a function $k: \mathcal{X} \times \mathcal{X} \rightarrow \Reals$ that is
\begin{enumerate}
  \item symmetric $k(x, x') = k(x', x)$, and
  \item satisfies for all $x_i$, $x_j \in \mathcal{X}$ and $c \in \Reals$: $\sum_{i}\sum_j c_i c_j k(x_i, x_j)\ge 0$.
\end{enumerate}
\end{definition}
One of the simplest examples of a p.d. kernel is $k(x, x') = xx'$, where $\mathcal{X} = \Reals$. This kernel is often called the linear kernel. While being simple, as we will see through the next few theorems, the associated space of functions induced by this kernel is often too restricte
In the next theorems we will see how kernels characterise a space over functions. Some of these spaces will be very explicit. For example, we will show how the linear kernel leads to a space of functions that are also linear, i.e. of the form $x \mapsto a x + b$. Other spaces belonging to kernel will be defined in a more abstract way. 

\begin{theorem}[Aronszjan, 1950 TODO]
  \label{theorem:k-and-H}
  The kernel $k$ is a p.d. kernel on $\mathcal{X}$ i.f.f. there exists a Hilbert space $\rkhs$ and a mapping $\phi: \mathcal{X} \rightarrow \rkhs$ such that $\forall x,x' \in \mathcal{X}$:
  \begin{equation}
    k(x, x') = \langle \phi(x), \phi(x') \rangle_{\rkhs}
  \end{equation}
\end{theorem}

We want to remind the reader that a Hilbert space $\rkhs$ is a (possibly infinite) vector space with an inner product $\langle f, g \rangle_{\rkhs}$ and norm $\norm{f} = \langle f, f \rangle_{\rkhs}^{\frac{1}{2}}$, and where any Cauchy sequence in $\rkhs$ converges in $\rkhs$. A Cauchy sequence is a sequence whose elements become arbitrarly close as the sequence progresses. Intuitively, a Hilbert space can be seen as a generalisation of an Euclidian space that is infinite dimensional. In the machine learning literature, $\rkhs$ is also commonly referred to as the feature space and $\phi$ the feature map.

A Reproducing Kernel Hilbert Space (RKHS) is a special type of Hilbert space mentioned in \cref{theorem:k-and-H}. It is a space over function from $\mathcal{X}$ to $\Reals$ that are ``parameterised'' by an element in $\mathcal{X}$. In other words, a datapoint $x \in \mathcal{X}$ is mapped to a function in the RKHS: $\phi(x) \in \rkhs$. To make it more explicit we also write $\phi(x) = k_x$ to highlight that $k_x$ is another function with domains $\mathcal{X} \rightarrow \Reals$.

\begin{definition}
  Let $\mathcal{X}$ be a set and $\rkhs$ a Hilbert space with inner-product $\langle \cdot, \cdot \rangle_{\rkhs}$. The function $k: \mathcal{X} \times \mathcal{X} \rightarrow \Reals$ is called a reproducing kernel of $\rkhs$ if
  \begin{enumerate}
    \item $\rkhs$ contains all functions of the form 
    \item 
  \end{enumerate}
\end{definition}

\begin{enumerate}
  \item History: RKHS pure math and into machine learning for SVM, splines and 
  \item Positive Definite and Symmetry
  \item RKHS
  \item Bochner's theorem
  \item Mercer Decomposition
  \item Examples of RKHS
  \item RKHS through Spectral Decomposition
  \item Representer Theorem
  \item Show how sparse approximation links anchor points
\end{enumerate}

\section{Spectral Representation of Gaussian Processes}
